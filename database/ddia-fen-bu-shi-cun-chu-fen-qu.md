# DDIA-分布式存储-分区

### 什么是分区

### 分区的目的

分区的目标是将数据和查询负载均匀的分布到各个节点上

#### 思考

1. 索引如何与分区配合
2. 分区如何实现可伸缩
3. 分区与复制的关系

首先第三个问题，分区和复制其实没有直接关系，但是通常会结合使用，使得每个分区的副本可以存储在多个节点上，用以获得更高的容错能力

### 分区的方式

#### 按键分区

**根据键范围进行分区** 把健的取值范围进行分段（就像英文词典的 a-z 目录一样），每个分区负责存储某个分段的数据 在这种存储方式下，可以按照一定的顺序来保存键值，这让我们可以很方便的进行范围扫描。但这样的方式也同样存在一些问题，比如在一些特定场景下的热点问题（如按时间戳顺序存储，那么最近的时间戳所存在的节点将会成为热点） 当然，我们可以复合更多的信息到分区键上（像联合索引那样），比如时间戳+用户ID个数位，但是这样一来当需要获得 time-range 数据时就需要进行多次查询进行合并

**根据键散列值进行分区** 许多分布式存储使用散列函数（hash）结果来确定分区，以此来规避数据偏斜和热点风险 我们可以给每个分区分配一个散列范围，这样可以有助于后续分裂 or rebalance 操作（一致性哈希思路） 通过散列进行分区的缺点是，这样就不能进行高效的范围查询操作，范围查询将需要聚集所有分区的查询结果

#### 次级索引

次级索引是关系型数据库的基础 次级索引的问题是：它们不能整齐的映射到分区 两种常见方式：

1. 基于文档的分区（document-based）
2. 基于关键词的分区（term-based）

思考：

* 本地索引和全局索引
* 全局索引的分区问题，索引数据更新问题

### Rebalance

需要 rebalance 的场景有很多，比如： 1. 需要横向扩展来增加负载和吞吐 2. 需要收缩来减配降耗 3. 节点故障导致的节点机器切换

一般 rebalance 需要能够满足一些要求： 1. rebalance 后，流量&存储负载均匀 2. rebalance 过程应该是平滑不停机的，整体服务需要能够继续接受处理请求 3. 应该避免全部数据的 rebalance（最简单的像一致性哈希思路）

常见的几种辅助 rebalance 的分区方案： 1. 固定分区数量 2. 动态分区 3. 按节点比例分区

#### 固定分区数量

就像标题一样，这种方法很简单，就是创建比节点数量多的固定数量的分区，并把这些分区均匀地分布在所有节点上

当新加节点的时候，总分区数量不变，而是去现有节点中窃取分区来达到再平衡。删除节点时则反之。

这种方式的特点： 1. 总分区数在创建时固定，不再增减 2. 只有分区在不同节点上移动，key 对应的分区不变 3. 初始的总分区数也代表了最大节点数 4. 分区在节点间移动不是即时的，在此期间原分区需要能够接受请求

综上所述，这种方式下选定合适的初始分区数量是非常重要的 但是在数据集的总大小难以预估的情况下，合理选择是困难的

#### 动态分区

在使用键范围分区的情境下，固定分区数量是非常不便的 `HBase` 这种按键范围分区的数据库会使用动态分区方案，在 Hbase 中分区文件的传输是通过 HDFS 来实现的

动态分区的主要思路是：

* 每个分区数据会有一组 min, max 限制
* 增长超过 max 限制，将会触发分裂
* 删除等导致小于 min 限制，将会触发合并

动态分区最大的优点就是分区数量适应总数据量

#### 按节点比例分区

这种方式使分区数和节点数成正比，也就是说每个节点上有固定数量的分区 当增加节点的时候，该节点会建立同样固定数量的分区，并从已有分区上拆分数据过来，反之亦然

总结 每个分区理论上都是独立运行的（分布式可扩展性的基础），不过这也同样导致了麻烦，比如需要同时写入多个分区时，由于每个分区独立运行，那么保证整个操作符合预期将变得困难，这也就引入了分布式事务的概念

